<!DOCTYPE html>
<html lang="kr">

<!-- Head tag -->
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="RLKorea ë¸”ë¡œê·¸">
    

    <!--Author-->
    
        <meta name="author" content="Bomi Yu">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Deterministic Policy Gradient Algorithms"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="RLKorea ë¸”ë¡œê·¸" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="RLKorea"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Deterministic Policy Gradient Algorithms - RLKorea</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/monokai-sublime.min.css">
    <link rel="stylesheet" href="/css/customizing.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125432253-1', 'auto');
        ga('send', 'pageview');

    </script>



    <!-- favicon -->
    
    <link rel="icon" href="/img/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
	
</head>


<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Menu -->
    <!-- Navigation -->
<div class="custom-nav">
	<div class="menu-icon">
		<button>
			<span class="bar"></span>
			<span class="bar"></span>
			<span class="bar"></span>			
		</button>
	</div>
	<a class="logo" href="/">RLKorea</a>

</div>	

<div class="left-side on" id="bs-example-navbar-collapse-1">
	<button class="close">
		<span class="bar"></span>
		<span class="bar"></span>
	</button>
	<div class="intro">
		<a href="/"><img src="/img/logo.png" alt="ë‚´ì‚¬ì§„"></a>
		<p><i class="fa fa-facebook"></i><a href="https://www.facebook.com/groups/ReinforcementLearningKR/"><strong> RLKorea</strong></a><br>
			RLKorea ë¸”ë¡œê·¸</p>
	</div>
	<ul>
		<!--<li><a href="/2018/05/16/RLKorea_intro/">RLKoreaì†Œê°œ</a></li> -->
		<li class="more_btn">
			<p class="clearfix">
			    <span class="left">
			        í”„ë¡œì íŠ¸
			    </span>
			    <i class="fa fa-sort-down right"></i>
			</p>
			<ul>
				<li><a href="/2018/06/29/0_pg-travel-guide/">í”¼ì§€ì—¬í–‰</a></li>
				<li><a href="/tags/ì•ŒíŒŒì˜¤ëª©/">ì•ŒíŒŒì˜¤ëª©</a></li>
				<li><a href="/2018/09/27/Distributional_intro/">Dist_RL</a></li>
				<li><a href="/2018/11/20/robot_arm_intro/">ê°ì¡ê³ ë¡œë´‡íŒ”</a></li>
				<li><a href="/2019/01/22/0_lets-do-irl-guide/">GAILí•˜ì!</a></li>
				<!--<li><a href="/tags/í™ˆë„¤ë¹„/">í™ˆë„¤ë¹„</a></li>
				<li><a href="/tags/DQN ë½€ê°œê¸°/">DQN ë½€ê°œê¸°</a></li>
				<li><a href="/tags/ê´‘ì‚°ê¹¨ê¸°/">ê´‘ì‚°ê¹¨ê¸°</a></li>-->
			</ul>
		</li>
		<li class="more_btn"><a href="#">ë…¼ë¬¸ë¦¬ë·°</a></li>
		<li class="more_btn"><a href="#">ê°•í™”í•™ìŠµ ê´€ë ¨ ì»¨í…ì¸ </a></li>
		<li class="more_btn"><a href="#">ê°•í™”í•™ìŠµ ê´€ë ¨ Q&A</a></li>
		<!-- 
			<li>
				<a href="/">
					
						Home
					
				</a>
			</li>
		
			<li>
				<a href="/archives">
					
						Archives
					
				</a>
			</li>
		
			<li>
				<a href="/tags">
					
						Tags
					
				</a>
			</li>
		
			<li>
				<a href="/categories">
					
						Categories
					
				</a>
			</li>
		
			<li>
				<a href="https://github.com/reinforcement-learning-kr">
					
						<i class="fa fa-github fa-stack-2x"></i>
					
				</a>
			</li>
		 -->
	</ul>
</div>



    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header post">
	<div class="container">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<div class="post-heading">
					<h1>Deterministic Policy Gradient Algorithms</h1>
					
					<h2 class="post-subheading">
						í”¼ì§€ì—¬í–‰ 2ë²ˆì§¸ ë…¼ë¬¸
					</h2>
					
					<span class="meta">
						<!-- Date and Author -->
						
							Posted by ê¹€ë™ë¯¼, ê³µë¯¼ì„œ, ì¥ìˆ˜ì˜, ì°¨ê¸ˆê°• on
						
						
							2018-06-27
						
					</span>
				</div>
			</div>
		</div>
	</div>
</header>

<!-- Post Content -->
<article>
	<div class="container">
		<div class="row">

			<!-- Tags and categories -->
		   
				<div class="col-lg-5 col-lg-offset-2 col-md-6 col-md-offset-1 post-tags">
					
						


<a href="/tags/í”„ë¡œì íŠ¸/">#í”„ë¡œì íŠ¸</a> <a href="/tags/í”¼ì§€ì—¬í–‰/">#í”¼ì§€ì—¬í–‰</a>


					
				</div>
				<div class="col-lg-3 col-md-4 post-categories">
					
						

<a href="/categories/í”„ë¡œì íŠ¸/">í”„ë¡œì íŠ¸</a>

					
				</div>
			

			<!-- Gallery -->
			

			<!-- Post Main Content -->
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<center> <img src="https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1" width="800"> </center>

<p>ë…¼ë¬¸ ì €ì : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>ë…¼ë¬¸ ë§í¬ : <a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">main text</a>, <a href="http://proceedings.mlr.press/v32/silver14-supp.pdf" target="_blank" rel="noopener">supplementary material</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>ì •ë¦¬ : ê¹€ë™ë¯¼, ê³µë¯¼ì„œ, ì¥ìˆ˜ì˜, ì°¨ê¸ˆê°•</p>
<hr>
<h1 id="1-ë“¤ì–´ê°€ë©°â€¦"><a href="#1-ë“¤ì–´ê°€ë©°â€¦" class="headerlink" title="1. ë“¤ì–´ê°€ë©°â€¦"></a>1. ë“¤ì–´ê°€ë©°â€¦</h1><ul>
<li>Deterministic Policy Gradient (DPG) Theoremì„ ì œì•ˆí•©ë‹ˆë‹¤.<ul>
<li>ì¤‘ìš”í•œ ì ì€ DPGëŠ” Expected gradient of the action-value functionì˜ í˜•íƒœë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li>Policy varianceê°€ 0ì— ìˆ˜ë ´í•  ê²½ìš°, DPGëŠ” Stochastic Policy Gradient (SPG)ì™€ ë™ì¼í•´ì§‘ë‹ˆë‹¤. (Theorem 2)<ul>
<li>Theorem 2ë¡œ ì¸í•´ ê¸°ì¡´ Policy Gradient (PG) ì™€ ê´€ë ¨ëœ ê¸°ë²•ë“¤ì„ DPGì— ì ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.<ul>
<li>ì˜ˆ. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>
</ul>
</li>
</ul>
</li>
<li>ì ì ˆí•œ exploration ì„ ìœ„í•´ model-free, off-policy actor-critic algorithm ì„ ì œì•ˆí•©ë‹ˆë‹¤<ul>
<li>action-value function approximator ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ policy gradientê°€ biasë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ compatibility conditionì„ ì œê³µí•©ë‹ˆë‹¤. (Theorem 3)</li>
</ul>
</li>
<li>DPG ëŠ” SPG ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.<ul>
<li>íŠ¹íˆ high dimensional action spacesë¥¼ ê°€ì§€ëŠ” tasksì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒì´ í½ë‹ˆë‹¤.<ul>
<li>SPGì˜ policy gradientëŠ” stateì™€ action spaces ëª¨ë‘ì— ëŒ€í•´ì„œ, DPGì˜ policy gradientëŠ” state spacesì— ëŒ€í•´ì„œë§Œ í‰ê· ì„ ì·¨í•©ë‹ˆë‹¤.</li>
<li>ê²°ê³¼ì ìœ¼ë¡œ, action spacesì˜ dimensionì´ ì»¤ì§ˆìˆ˜ë¡ data efficiencyê°€ ë†’ì€ DPGì˜ í•™ìŠµì´ ë” ì˜ ì´ë¤„ì§€ê²Œ ë©ë‹ˆë‹¤.</li>
<li>ë¬´í•œì • í•™ìŠµì„ ì‹œí‚¤ë©´, SPGë„ ìµœì ìœ¼ë¡œ ìˆ˜ë ´í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ê¸°ì— ìœ„ ì„±ëŠ¥ ë¹„êµëŠ” ì¼ì • iteration ë‚´ë¡œ í•œì •í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ê¸°ì¡´ ê¸°ë²•ë“¤ì— ë¹„í•´ computation ì–‘ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.<ul>
<li>Computation ì€ action dimensionality ì™€ policy parameters ìˆ˜ì— ë¹„ë¡€í•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br></p>
<h1 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h1><p><br></p>
<h2 id="2-1-Performance-objective-function"><a href="#2-1-Performance-objective-function" class="headerlink" title="2.1 Performance objective function"></a>2.1 Performance objective function</h2><p>$$<br>\begin{align}<br>J(\pi_{\theta}) &amp;= \int_{S}\rho^{\pi}(s)\int_{A}\pi_{\theta}(s,a)r(s,a)da ds = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[r(s,a)]<br>\end{align}<br>$$</p>
<p><br></p>
<h2 id="2-2-SPG-Theorem"><a href="#2-2-SPG-Theorem" class="headerlink" title="2.2 SPG Theorem"></a>2.2 SPG Theorem</h2><ul>
<li>State distribution $ \rho^{\pi}(s) $ ì€ policy parametersì— ì˜í–¥ì„ ë°›ì§€ë§Œ, policy gradient ë¥¼ ê³„ì‚°í•  ë•ŒëŠ” state distribution ì˜ gradient ë¥¼ ê³ ë ¤í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.<br>$$\begin{eqnarray}\nabla_{\theta}J(\pi_{\theta}) &amp;=&amp; \int_{S}\rho^{\pi}(s)\int_{A}\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \ &amp;=&amp; E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s)Q^{\pi}(s,a)]<br>\end{eqnarray}$$</li>
</ul>
<p><br></p>
<h2 id="2-3-Stochastic-Actor-Critic-Algorithms"><a href="#2-3-Stochastic-Actor-Critic-Algorithms" class="headerlink" title="2.3 Stochastic Actor-Critic Algorithms"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>
<li>Actorì™€ Criticì´ ë²ˆê°ˆì•„ê°€ë©´ì„œ ë™ì‘í•˜ë©° stochastic policyë¥¼ ìµœì í™”í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.</li>
<li>Actor: $ Q^{\pi}(s,a) $ ë¥¼ ê·¼ì‚¬í•œ $ Q^w(s,a) $ë¥¼ ì´ìš©í•´ stochastic policy gradientë¥¼ ascentí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ policy parameter $ \theta $ë¥¼ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨ stochastic policyë¥¼ ë°œì „ì‹œí‚µë‹ˆë‹¤.<ul>
<li>$ \nabla_{\theta}J(\pi_{\theta}) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s)Q^{w}(s,a)] $</li>
</ul>
</li>
<li>Critic: SARSAë‚˜ Q-learningê°™ì€ Temporal-difference (TD) learningì„ ì´ìš©í•´ action-value functionì˜ parameter, $ w $ë¥¼ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨ $ Q^w(s,a) $ê°€ $ Q^{\pi}(s,a) $ê³¼ ìœ ì‚¬í•´ì§€ë„ë¡ í•©ë‹ˆë‹¤.</li>
<li>ì‹¤ì œ ê°’ì¸ $ Q^{\pi}(s,a) $ ëŒ€ì‹  ì´ë¥¼ ê·¼ì‚¬í•œ $ Q^w(s,a) $ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ì¼ë°˜ì ìœ¼ë¡œ biasê°€ ë°œìƒí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, compatible conditionì— ë¶€í•©í•˜ëŠ” $ Q^w(s,a) $ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, biasê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</li>
</ul>
<p><br></p>
<h2 id="2-4-Off-policy-Actor-Critic"><a href="#2-4-Off-policy-Actor-Critic" class="headerlink" title="2.4 Off-policy Actor-Critic"></a>2.4 Off-policy Actor-Critic</h2><ul>
<li>Distinct behavior policy $ \beta(a|s) ( \neq \pi_{\theta}(a|s) ) $ ë¡œë¶€í„° ìƒ˜í”Œë§ëœ trajectories ë¥¼ ì´ìš©í•œ Actor-Critic</li>
<li>Performance objective function<ul>
<li>$\begin{eqnarray}<br>  J_{\beta}(\pi_{\theta})<br>  &amp;=&amp; \int_{S}\rho^{\beta}(s)V^{\pi}(s)ds \nonumber \\<br>  &amp;=&amp; \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads<br>  \end{eqnarray} $</li>
</ul>
</li>
<li>off-policy policy gradient<ul>
<li>$ \begin{eqnarray}<br>  \nabla_{\theta}J_{\beta}(\pi_{\theta}) &amp;\approx&amp; \int_{S}\int_{A}\rho^{\beta}(s)\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \end{eqnarray} $<br>  $=E_{s \sim \rho^{\beta}, a \sim \beta}[\frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)}\nabla_{\theta}\log\pi_{\theta}(a|s)Q^{\pi}(s,a)]$</li>
<li>off-policy policy gradient ì‹ì—ì„œì˜ ë¬¼ê²° í‘œì‹œëŠ” <a href="https://arxiv.org/abs/1205.4839" target="_blank" rel="noopener">Degris, 2012b</a> ë…¼ë¬¸ì— ê·¼ê±°í•©ë‹ˆë‹¤.<ul>
<li>Exact off-policy policy gradient ì™€ ì´ë¥¼ approximate í•œ policy gradient ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (ë¹¨ê°„ìƒ‰ ìƒìì— ìˆëŠ” í•­ëª©ì„ ì‚­ì œí•¨ìœ¼ë¡œì¨ ê·¼ì‚¬í•©ë‹ˆë‹¤.)<ul>
<li><img src="https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1" width="500px"></li>
</ul>
</li>
<li>[Degris, 2012b] Theorem 1 ì— ì˜í•´ policy parameter ê°€ approximated policy gradient ( $\nabla_{u}ğ‘„^{\pi,\gamma}(ğ‘ ,ğ‘)$ term ì œê±°)ì— ë”°ë¼ ì—…ë°ì´íŠ¸ë˜ì–´ë„ policy ëŠ” improve ê°€ ë¨ì´ ë³´ì¥ë˜ê¸°ì— exact off-policy policy gradient ëŒ€ì‹  approximated off-policy policy gradient ë¥¼ ì‚¬ìš©í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.<ul>
<li><img src="https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1" width="500px"></li>
</ul>
</li>
</ul>
</li>
<li>off-policy policy gradient ì‹ì—ì„œ $ \frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)} $ëŠ” importance sampling ratio ì…ë‹ˆë‹¤.<ul>
<li>off-policy actor-criticì—ì„œëŠ” $ \beta $ì— ì˜í•´ ìƒ˜í”Œë§ëœ trajectoryë¥¼ ì´ìš©í•´ì„œ stochastic policy $ \pi $ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— importance samplingì´ í•„ìš”í•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br></p>
<h1 id="3-Gradient-of-Deterministic-Policies"><a href="#3-Gradient-of-Deterministic-Policies" class="headerlink" title="3. Gradient of Deterministic Policies"></a>3. Gradient of Deterministic Policies</h1><p><br></p>
<h2 id="3-1-Regularity-Conditions"><a href="#3-1-Regularity-Conditions" class="headerlink" title="3.1 Regularity Conditions"></a>3.1 Regularity Conditions</h2><ul>
<li>ì–´ë– í•œ ì´ë¡ ì´ ì„±ë¦½í•˜ê¸° ìœ„í•œ ì „ì œ ì¡°ê±´</li>
<li>Regularity conditions A.1<ul>
<li>$ p(sâ€™|s,a), \nabla_{a}p(sâ€™|s,a), \mu_{\theta}(s), \nabla_{\theta}\mu_{\theta}(s), r(s,a), \nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, sâ€™ $ and $ x $.</li>
</ul>
</li>
<li>regularity conditions A.2<ul>
<li>There exists a $ b $ and $ L $ such that $ \sup_{s}p_{1}(s) &lt; b $, $ \sup_{a,s,sâ€™}p(sâ€™|s,a) &lt; b $, $ \sup_{a,s}r(s,a) &lt; b $, $ \sup_{a,s,sâ€™}|\nabla_{a}p(sâ€™|s,a)| &lt; L $, and $ \sup_{a,s}|\nabla_{a}r(s,a)| &lt; L $.</li>
</ul>
</li>
</ul>
<p><br></p>
<h2 id="3-2-Deterministic-Policy-Gradient-Theorem"><a href="#3-2-Deterministic-Policy-Gradient-Theorem" class="headerlink" title="3.2 Deterministic Policy Gradient Theorem"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>
<li>Deterministic policy<ul>
<li>$ \mu_{\theta} : S \to A $ with parameter vector $ \theta \in \mathbb{R}^n $</li>
</ul>
</li>
<li>Probability distribution<ul>
<li>$ p(s \to sâ€™, t, \mu) $</li>
</ul>
</li>
<li>Discounted state distribution<ul>
<li>$ \rho^{\mu}(s) $</li>
</ul>
</li>
<li>Performance objective</li>
</ul>
<p>$$<br>J(\mu_{\theta}) = E[r^{\gamma}_{1} | \mu]<br>$$</p>
<p>$$<br>= \int_{S}\rho^{\mu}(s)r(s,\mu_{\theta}(s))ds<br>= E_{s \sim \rho^{\mu}}[r(s,\mu_{\theta}(s))]<br>$$</p>
<ul>
<li><p>DPG Theorem</p>
<ul>
<li><p>MDP ê°€ A.1 ë§Œì¡±í•œë‹¤ë©´, ì•„ë˜ ì‹ì´ ì„±ë¦½í•©ë‹ˆë‹¤.<br>$\nabla_{\theta}J(\mu_{\theta}) = \int_{S}\rho^{\mu}(s)\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}ds \nonumber$<br>$= E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]   \nonumber $ </p>
</li>
<li><p>DPGëŠ” state spaceì— ëŒ€í•´ì„œë§Œ í‰ê· ì„ ì·¨í•˜ë©´ ë˜ê¸°ì—, stateì™€ action space ëª¨ë‘ì— ëŒ€í•´ í‰ê· ì„ ì·¨í•´ì•¼ í•˜ëŠ” SPGì— ë¹„í•´ data efficiencyê°€ ì¢‹ìŠµë‹ˆë‹¤. ì¦‰, ë” ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ í•™ìŠµì´ ì˜ ì´ë¤„ì§€ê²Œ ë©ë‹ˆë‹¤.</p>
</li>
</ul>
</li>
</ul>
<p><br>    </p>
<h2 id="3-3-DPG-í˜•íƒœì—-ëŒ€í•œ-informal-intuition"><a href="#3-3-DPG-í˜•íƒœì—-ëŒ€í•œ-informal-intuition" class="headerlink" title="3.3 DPG í˜•íƒœì— ëŒ€í•œ informal intuition"></a>3.3 DPG í˜•íƒœì— ëŒ€í•œ informal intuition</h2><ul>
<li>Generalized policy iteration<ul>
<li>ì •ì±… í‰ê°€ì™€ ì •ì±… ë°œì „ì„ í•œ ë²ˆ ì”© ë²ˆê°ˆì•„ ê°€ë©´ì„œ ì‹¤í–‰í•˜ëŠ” ì •ì±… iteration<ul>
<li>ìœ„ì™€ ê°™ì´ í•´ë„ ì •ì±… í‰ê°€ì—ì„œ ì˜ˆì¸¡í•œ ê°€ì¹˜í•¨ìˆ˜ê°€ ìµœì  ê°€ì¹˜í•¨ìˆ˜ì— ìˆ˜ë ´í•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>ì •ì±… í‰ê°€<ul>
<li>action-value function $ Q^{\pi}(s,a) $ or $ Q^{\mu}(s,a) $ì„ estimate í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ì •ì±… ë°œì „<ul>
<li>ìœ„ estimated action-value functionì— ë”°ë¼ ì •ì±…ì„ updateí•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.</li>
<li>ì£¼ë¡œ action-value functionì— ëŒ€í•œ greedy maximisationì„ ì‚¬ìš©í•©ë‹ˆë‹¤.<ul>
<li>$ \mu^{k+1}(s) = \arg\max\limits_{a}Q^{\mu^{k}}(s,a) $</li>
<li>greedy ì •ì±… ë°œì „ì€ ë§¤ ë‹¨ê³„ë§ˆë‹¤ global maximizationì„ í•´ì•¼í•˜ëŠ”ë°, ì´ë¡œ ì¸í•´ continuous action spacesì—ì„œ ê³„ì‚°ëŸ‰ì´ ê¸‰ê²©íˆ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ê·¸ë˜ì„œ policy gradient ë°©ë²•ì´ ë‚˜ì˜µë‹ˆë‹¤.<ul>
<li>policy ë¥¼ $ \theta $ì— ëŒ€í•´ì„œ parameterize í•©ë‹ˆë‹¤.</li>
<li>ë§¤ ë‹¨ê³„ë§ˆë‹¤ global maximisation ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ , ë°©ë¬¸í•˜ëŠ” state $ s $ë§ˆë‹¤ policy parameterë¥¼ action-value function $ Q $ì˜ $ \theta $ì— ëŒ€í•œ gradient $ \nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s)) $ ë°©í–¥ìœ¼ë¡œ proportionalí•˜ê²Œ update í•©ë‹ˆë‹¤.</li>
<li>í•˜ì§€ë§Œ ê° stateëŠ” ë‹¤ë¥¸ ë°©í–¥ì„ ì œì‹œí•  ìˆ˜ ìˆê¸°ì—, state distribution $ \rho^{\mu}(s) $ì— ëŒ€í•œ ê¸°ëŒ€ê°’ì„ ì·¨í•´ policy parameterë¥¼ update í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<ul>
<li>$ \theta^{k+1} = \theta^{k} + \alpha E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s))] $</li>
</ul>
</li>
<li>ì´ëŠ” chain-ruleì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ ë¶„ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<ul>
<li>$ \theta^{k+1} = \theta^{k} + \alpha E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu^{k}}(s,a)\vert_{a=\mu_{\theta}(s)}] $ (7)</li>
<li>chain rule: $ \frac{\partial Q}{\partial \theta} = \frac{\partial a}{\partial \theta} \frac{\partial Q}{\partial a} $</li>
</ul>
</li>
<li>í•˜ì§€ë§Œ state distribution $ \rho^{\mu} $ì€ ì •ì±…ì— dependent í•©ë‹ˆë‹¤.<ul>
<li>ì •ì±…ì´ ë°”ê¾¸ê²Œ ë˜ë©´, ë°”ë€ ì •ì±…ì— ë”°ë¼ ë°©ë¬¸í•˜ê²Œ ë˜ëŠ” stateê°€ ë³€í•˜ê¸° ë•Œë¬¸ì— state distributionì´ ë³€í•˜ê²Œ ë©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ê·¸ë ‡ê¸°ì— ì •ì±… update ì‹œ state distributionì— ëŒ€í•œ gradientë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë° ì •ì±… ë°œì „ì´ ì´ë¤„ì§„ë‹¤ëŠ” ê²ƒì€ ì§ê´€ì ìœ¼ë¡œ ì™€ë‹¿ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li>deterministic policy gradient theoremì€ state distributionì— ëŒ€í•œ gradient ê³„ì‚°ì—†ì´ ìœ„ ì‹(7) ëŒ€ë¡œë§Œ updateí•´ë„ performance objectiveì˜ gradientë¥¼ ì •í™•í•˜ê²Œ ë”°ë¦„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h2 id="3-4-DPGëŠ”-SPGì˜-limiting-case"><a href="#3-4-DPGëŠ”-SPGì˜-limiting-case" class="headerlink" title="3.4 DPGëŠ” SPGì˜ limiting case"></a>3.4 DPGëŠ” SPGì˜ limiting case</h2><ul>
<li>stochastic policy parameterization<ul>
<li>$ \pi_{\mu_{\theta},\sigma} $ by a deterministic policy $ \mu_{\theta} : S \to A $ and a variance parameter $ \sigma $</li>
<li>$ \sigma = 0 $ ì´ë©´, $ \pi_{\mu_{\theta},\sigma} \equiv \mu_{\theta} $</li>
</ul>
</li>
<li>Theorem 2. Policyì˜ varianceê°€ 0ì— ìˆ˜ë ´í•˜ë©´, ì¦‰, $ \sigma \to 0 $, stochastic policy gradientì™€ deterministic policy gradientëŠ” ë™ì¼í•´ì§‘ë‹ˆë‹¤.<ul>
<li>ì¡°ê±´: stochastic policy $ \pi_{\mu_{\theta},\sigma} = \nu_{\sigma}(\mu_{\theta}(s),a) $<ul>
<li>$ \sigma $ëŠ” varianceì…ë‹ˆë‹¤.</li>
<li>$ \nu_{\sigma}(\mu_{\theta}(s),a) $ëŠ” condition B.1ì„ ë§Œì¡±í•©ë‹ˆë‹¤.</li>
<li>MDPëŠ” conditions A.1ê³¼ A.2ë¥¼ ë§Œì¡±í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ê²°ê³¼:<ul>
<li>$ \lim\limits_{\sigma\downarrow0}\nabla_{\theta}J(\pi_{\mu_{\theta},\sigma}) = \nabla_{\theta}J(\mu_{\theta})  $<ul>
<li>ì¢Œë³€ì€ standard stochastic gradientì´ë©°, ìš°ë³€ì€ deterministic gradientì…ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>ì˜ë¯¸:<ul>
<li>deterministic policy gradientëŠ” stochastic policy gradientì˜ íŠ¹ìˆ˜ case ì…ë‹ˆë‹¤.</li>
<li>ê¸°ì¡´ ìœ ëª…í•œ policy gradients ê¸°ë²•ë“¤ì— deterministic policy gradients ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<ul>
<li>ê¸°ì¡´ ê¸°ë²•ë“¤ ì˜ˆ: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br></p>
<h1 id="4-Deterministic-Actor-Critic-Algorithms"><a href="#4-Deterministic-Actor-Critic-Algorithms" class="headerlink" title="4. Deterministic Actor-Critic Algorithms"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>
<li>SARSA criticë¥¼ ì´ìš©í•œ on-policy actor-critic<ul>
<li>ë‹¨ì <ul>
<li>deterministic policyì— ì˜í•´ í–‰ë™í•˜ë©´ explorationì´ ì˜ ë˜ì§€ ì•Šê¸°ì—, sub-optimalì— ë¹ ì§€ê¸° ì‰½ìŠµë‹ˆë‹¤.</li>
</ul>
</li>
<li>ëª©ì <ul>
<li>êµí›ˆ/ì •ë³´ì œê³µ</li>
<li>í™˜ê²½ì—ì„œ ì¶©ë¶„í•œ noiseë¥¼ ì œê³µí•˜ì—¬ explorationì„ ì‹œí‚¬ ìˆ˜ ìˆë‹¤ë©´, deterministic policyë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•˜ì—¬ë„ ì¢‹ì€ í•™ìŠµ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<ul>
<li>ì˜ˆ. ë°”ëŒì´ agentì˜ í–‰ë™ì— ì˜í–¥(noise)ì„ ì¤Œ</li>
</ul>
</li>
</ul>
</li>
<li>Remind: ì‚´ì‚¬(SARSA) update rule<ul>
<li>$ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>
</ul>
</li>
<li>Algorithm<ul>
<li>Criticì€ MSEë¥¼ $ \bf minimize $í•˜ëŠ” ë°©í–¥, ì¦‰, action-value functionì„ stochastic gradient $ \bf descent $ ë°©ë²•ìœ¼ë¡œ updateí•©ë‹ˆë‹¤.<ul>
<li>$ MSE = [Q^{\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>
<li>criticì€ ì‹¤ì œ $ Q^{\mu}(s,a) $ ëŒ€ì‹  ë¯¸ë¶„ ê°€ëŠ¥í•œ $ Q^{w}(s,a) $ë¡œ ëŒ€ì²´í•˜ì—¬ action-value functionì„ estimateí•˜ë©°, ì´ ë‘˜ ê°„ mean square errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li>$ \nabla_{w}MSE \approx -2 * [r + \gamma Q^{w}(sâ€™,aâ€™) - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $<ul>
<li>$ \nabla_{w}MSE = -2 * [Q^{\mu}(s,a) - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $</li>
<li>$ Q^{\mu}(s,a) $ ë¥¼ $ r + \gamma Q^{w}(sâ€™,aâ€™) $ë¡œ ëŒ€ì²´<ul>
<li>$ Q^{\mu}(s,a) = r + \gamma Q^{\mu}(sâ€™,aâ€™) $</li>
</ul>
</li>
</ul>
</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>
<li>$w_{t+1} = w_{t} - \alpha_{w}\nabla_{w}MSE  \nonumber$<br>$ \approx w_{t} - \alpha_{w} <em> (-2 </em> [r + \gamma Q^{w}(sâ€™,aâ€™) - Q^{w}(s,a)] \nabla_{w}Q^{w}(s,a)$</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>
</ul>
</li>
</ul>
</li>
<li>ActorëŠ” ì‹(9)ì— ë”°ë¼ ë³´ìƒì´ $ \bf maximize $ë˜ëŠ” ë°©í–¥, ì¦‰, deterministic policyë¥¼ stochastic gradient $ \bf ascent $ ë°©ë²•ìœ¼ë¡œ updateí•©ë‹ˆë‹¤.<ul>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Q-learning ì„ ì´ìš©í•œ off-policy actor-critic<ul>
<li>stochastic behavior policy $ \beta(a|s) $ì— ì˜í•´ ìƒì„±ëœ trajectoriesë¡œë¶€í„° deterministic target policy $ \mu_{\theta}(s) $ë¥¼ í•™ìŠµí•˜ëŠ” off-policy actor-criticì…ë‹ˆë‹¤</li>
<li>performance objective<ul>
<li>$ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)V^{\mu}(s)ds \nonumber \\$<br>$= \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds \nonumber \\$<br>$= E_{s \sim \rho^{\beta}}[Q^{\mu}(s,\mu_{\theta}(s))]$</li>
</ul>
</li>
<li>off-policy deterministic policy gradient<ul>
<li>$ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $<ul>
<li>ë…¼ë¬¸ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‚˜ì™€ìˆëŠ”ë°, ë¬¼ê²° í‘œì‹œ ë¶€ë¶„ì€ ì˜¤ë¥˜ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.</li>
<li>$ \begin{eqnarray}<br>  \nabla_{\theta}J_{\beta}(\mu_{\theta}) &amp;\approx&amp; \int_{S}\rho^{\beta}(s)\nabla_{\theta}\mu_{\theta}(a|s)Q^{\mu}(s,a)ds \nonumber \<br>  &amp;=&amp; E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]<br>  \end{eqnarray} $</li>
<li>ê·¼ê±°: Actionì´ deterministicí•˜ê¸°ì— stochastic ê²½ìš°ì™€ëŠ” ë‹¤ë¥´ê²Œ performance objectiveì—ì„œ actionì— ëŒ€í•´ í‰ê· ì„ êµ¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ë ‡ê¸°ì—, ê³±ì˜ ë¯¸ë¶„ì´ ìˆì„ í•„ìš”ê°€ ì—†ê³ , [Degris, 2012b]ì—ì„œ ì²˜ëŸ¼ ê³±ì˜ ë¯¸ë¶„ì„ í†µí•´ ìƒê¸°ëŠ” action-value functionì— ëŒ€í•œ gradient termì„ ìƒëµí•  í•„ìš”ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>Remind: íëŸ¬ë‹(Q-learning) update rule<ul>
<li>$ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma \max\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>
</ul>
</li>
<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>
<li>ì‚´ì‚¬ë¥¼ ì´ìš©í•œ on-policy deterministic actor-criticê³¼ ì•„ë˜ ë¶€ë¶„ì„ ì œì™¸í•˜ê³ ëŠ” ê°™ìŠµë‹ˆë‹¤.<ul>
<li>target policyëŠ” $ \beta(a|s) $ì— ì˜í•´ ìƒì„±ëœ trajectoriesë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.</li>
<li>ì—…ë°ì´íŠ¸ ëª©í‘œ ë¶€ë¶„ì— ì‹¤ì œ í–‰ë™ ê°’ $ a_{t+1} $ì´ ì•„ë‹ˆë¼ ì •ì±…ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ í–‰ë™ ê°’ $ \mu_{\theta}(s_{t+1}) $ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.<ul>
<li>$ \mu_{\theta}(s_{t+1}) $ ì€ ê°€ì¥ ë†’ì€ Q ê°’ì„ ê°€ì§€ëŠ” í–‰ë™. ì¦‰, Q-learning.</li>
</ul>
</li>
</ul>
</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $</li>
</ul>
</li>
<li>Stochastic off-policy actor-criticì€ ëŒ€ê°œ actorì™€ critic ëª¨ë‘ importance samplingì„ í•„ìš”ë¡œ í•˜ì§€ë§Œ, deterministic policy gradientì—ì„  importance samplingì´ í•„ìš”ì—†ìŠµë‹ˆë‹¤.<ul>
<li>Actor ëŠ” deterministic ì´ê¸°ì— sampling ìì²´ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.<ul>
<li>Stochastic policyì¸ ê²½ìš°, Actorì—ì„œ importance samplingì´ í•„ìš”í•œ ì´ìœ ëŠ” ìƒíƒœ $ s $ì—ì„œì˜ ê°€ì¹˜ í•¨ìˆ˜ ê°’ $ V^{\pi}(s) $ì„ estimateí•˜ê¸° ìœ„í•´ $ \pi $ê°€ ì•„ë‹ˆë¼ $ \beta $ì— ë”°ë¼ samplingì„ í•œ í›„, í‰ê· ì„ ë‚´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</li>
<li>í•˜ì§€ë§Œ deterministic policyì¸ ê²½ìš°, ìƒíƒœ $ s $ì—ì„œì˜ ê°€ì¹˜ í•¨ìˆ˜ ê°’ $ V^{\pi}(s) = Q^{\pi}(s,\mu_{\theta}) $ ì¦‰, actionì´ ìƒíƒœ sì— ëŒ€í•´ deterministicì´ê¸°ì— samplingì„ í†µí•´ estimateí•  í•„ìš”ê°€ ì—†ê³ , ë”°ë¼ì„œ importance samplingë„ í•„ìš”ì—†ì–´ì§‘ë‹ˆë‹¤.</li>
<li>stochastic vs. deterministic performance objective<ul>
<li>stochastic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads $</li>
<li>deterministic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds $</li>
</ul>
</li>
</ul>
</li>
<li>Criticì´ ì‚¬ìš©í•˜ëŠ” Q-learningì€ importance samplingì´ í•„ìš”ì—†ëŠ” off policy ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.<ul>
<li>Q-learningë„ ì—…ë°ì´íŠ¸ ëª©í‘œë¥¼ íŠ¹ì • ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ì„ í†µí•´ estimate í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Q í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ê¸°ì— ìœ„ actor ì—ì„œì˜ deterministic ê²½ìš°ì™€ ë¹„ìŠ·í•˜ê²Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>compatible function approximation ë° gradient temporal-difference learning ì„ ì´ìš©í•œ actor-critic<ul>
<li>ìœ„ ì‚´ì‚¬/Q-learning ê¸°ë°˜ on/off-policyëŠ” ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œê°€ ì¡´ì¬í•©ë‹ˆë‹¤.<ul>
<li>function approximatorì— ì˜í•œ bias<ul>
<li>ì¼ë°˜ì ìœ¼ë¡œ $ Q^{\mu}(s,a) $ ë¥¼ $ Q^{w}(s,a) $ë¡œ ëŒ€ì²´í•˜ì—¬ deterministic policy gradientë¥¼ êµ¬í•˜ë©´, ê·¸ gradientëŠ” ascentí•˜ëŠ” ë°©í–¥ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
</li>
<li>off-policy learningì— ì˜í•œ instabilities</li>
</ul>
</li>
<li>ê·¸ë˜ì„œ stochasticì²˜ëŸ¼ $ \nabla_{a}Q^{\mu}(s,a) $ë¥¼ $ \nabla_{a}Q^{w}(s,a) $ë¡œ ëŒ€ì²´í•´ë„ deterministic policy gradientì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ compatible function approximator $ Q^{w}(s,a) $ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.</li>
<li>Theorem 3. ì•„ë˜ ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´, $ Q^{w}(s,a) $ëŠ” deterministic policy $ \mu_{\theta}(s) $ì™€ compatible í•©ë‹ˆë‹¤. ì¦‰, $ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)}] $<ul>
<li>$ \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $ì…ë‹ˆë‹¤.</li>
<li>$ w $ëŠ” $ MSE(\theta, w) = E[\epsilon(s;\theta,w)^{\top}\epsilon(s;\theta,w)] $ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.<ul>
<li>$ \epsilon(s;\theta,w) = \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} - \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}  $</li>
</ul>
</li>
</ul>
</li>
<li>Theorem 3ì€ on-policy ë¿ë§Œ ì•„ë‹ˆë¼ off-policyì—ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li>$ Q^{w}(s,a) = (a-\mu_{\theta}(s))^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top} w + V^{v}(s) $<ul>
<li>ì–´ë– í•œ deterministic policyì— ëŒ€í•´ì„œë„ ìœ„ í˜•íƒœì™€ ê°™ì€ compatible function approximatorê°€ ì¡´ì¬í•©ë‹ˆë‹¤.</li>
<li>ì•ì˜ termì€ advantageë¥¼, ë’¤ì˜ termì€ valueë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
</li>
<li>$ Q^{w}(s,a) = \phi(s,a)^{\top} w + v^{\top}\phi(s) $<ul>
<li>ì •ì˜ : $ \phi(s,a) \overset{\underset{\mathrm{def}}{}}{=} \nabla_{\theta}\mu_{\theta}(s)(a-\mu_{\theta}(s)) $</li>
<li>ì¼ë¡€ : $ V^{v}(s) = v^{\top}\phi(s) $</li>
<li>Theorem 3 ë§Œì¡± ì—¬ë¶€<ul>
<li>ì²« ë²ˆì§¸ ì¡°ê±´ ë§Œì¡±í•©ë‹ˆë‹¤.</li>
<li>ë‘ ë²ˆì§¸ ì¡°ê±´ì€ ëŒ€ê°• ë§Œì¡±í•©ë‹ˆë‹¤.<ul>
<li>$ \nabla_{a}Q^{\mu}(s,a) $ì— ëŒ€í•œ unbiased sampleì„ íšë“í•˜ê¸°ëŠ” ë§¤ìš° ì–´ë µê¸°ì—, ì¼ë°˜ì ì¸ ì •ì±… í‰ê°€ ë°©ë²•ë“¤ë¡œ $ w $ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.</li>
<li>ì´ ì •ì±… í‰ê°€ ë°©ë²•ë“¤ì„ ì´ìš©í•˜ë©´ $ Q^{w}(s,a) \approx Q^{\mu}(s,a) $ì¸ reasonable solutionì„ ì°¾ì„ ìˆ˜ ìˆê¸°ì— ëŒ€ê°• $ \nabla_{a}Q^{w}(s,a) \approx \nabla_{a}Q^{\mu}(s,a) $ì´ ë  ê²ƒì…ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>action-value functionì— ëŒ€í•œ linear function approximatorëŠ” í° ê°’ì„ ê°€ì§€ëŠ” actionsì— ëŒ€í•´ì„  divergeí•  ìˆ˜ ìˆì–´ globalí•˜ê²Œ action-values ì˜ˆì¸¡í•˜ê¸°ì—ëŠ” ì¢‹ì§€ ì•Šì§€ë§Œ, local criticì— ì‚¬ìš©í•  ë•ŒëŠ” ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.<ul>
<li>ì¦‰, ì ˆëŒ€ê°’ì´ ì•„ë‹ˆë¼ ì‘ì€ ë³€í™”ëŸ‰ì„ ë‹¤ë£¨ëŠ” gradient method ê²½ìš°ì—” $ A^{w}(s,\mu_{\theta}(s)+\delta) = \delta^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top}w $ë¡œ, divergeí•˜ì§€ ì•Šê³ , ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>
<li>Critic: ì‹¤ì œ action-value functionì— ëŒ€í•œ linear function approximatorì¸ $ Q^{w}(s,a) = \phi(s,a)^{\top}w $ë¥¼ estimateí•©ë‹ˆë‹¤.<ul>
<li>$ \phi(s,a) = a^{\top}\nabla_{\theta}\mu_{\theta} $</li>
<li>Behavior policy $ \beta(a|s) $ë¡œë¶€í„° ì–»ì€ samplesë¥¼ ì´ìš©í•˜ì—¬ Q-learningì´ë‚˜ gradient Q-learningê³¼ ê°™ì€ off-policy algorithmìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>Actor: estimated action-value functionì— ëŒ€í•œ gradientë¥¼ $ \nabla_{\theta}\mu_{\theta}(s)^{\top}w $ë¡œ ì¹˜í™˜ í›„, ì •ì±…ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $</li>
</ul>
</li>
<li>off-policy Q-learningì€ linear function approximationì„ ì´ìš©í•˜ë©´ diverge í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<ul>
<li>$ \mu_{\theta}(s_{t+1}) $ì´ divergeí•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.</li>
<li>ê·¸ë ‡ê¸°ì— simple Q-learning ëŒ€ì‹  ë‹¤ë¥¸ ê¸°ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>ê·¸ë ‡ê¸°ì— critic ì— gradient Q-learning ì‚¬ìš©í•œ COPDAC-GQ (Gradient Q-learning critic) algorithmì„ ì œì•ˆí•©ë‹ˆë‹¤.<ul>
<li>gradient temporal-difference learningì— ê¸°ë°˜í•œ ê¸°ë²•ë“¤ì€ true gradient descent algorithmì´ë©°, convergeê°€ ë³´ì¥ë©ë‹ˆë‹¤. (Sutton, 2009)<ul>
<li>ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” stochastic gradient descentë¡œ mean-squared projected Bellman error (MSPBE)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</li>
<li>criticì´ actorë³´ë‹¤ ë¹ ë¥¸ time-scaleë¡œ updateë˜ë„ë¡ step sizeë“¤ì„ ì˜ ì¡°ì ˆí•˜ë©´, criticì€ MSPBEë¥¼ ìµœì†Œí™”í•˜ëŠ” parametersë¡œ convergeí•˜ê²Œ ë©ë‹ˆë‹¤.</li>
<li>criticì— gradient temporal-difference learningì˜ ì¼ì¢…ì¸ gradient Q-learningì„ ì‚¬ìš©í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. (Maei, 2010)</li>
</ul>
</li>
</ul>
</li>
<li>COPDAC-GQ algorithm<ul>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) - \alpha_{w}\gamma\phi(s_{t+1}, \mu_{\theta}(s_{t+1}))(\phi(s_{t},a_{t})^{\top} u_{t}) $</li>
<li>$ v_{t+1} = v_{t} + \alpha_{v}\delta_{t}\phi(s_{t}) - \alpha_{v}\gamma\phi(s_{t+1})(\phi(s_{t},a_{t})^{\top} u_{t}) $</li>
<li>$ u_{t+1} = u_{t} + \alpha_{u}(\delta_{t}-\phi(s_{t}, a_{t})^{\top} u_{t})\phi(s_{t}, a_{t}) $</li>
</ul>
</li>
<li>stochastic actor-criticê³¼ ê°™ì´ ë§¤ time-step ë§ˆë‹¤ update ì‹œ í•„ìš”í•œ ê³„ì‚°ì˜ ë³µì¡ë„ëŠ” $ O(mn) $ì…ë‹ˆë‹¤.<ul>
<li>mì€ action dimensions, nì€ number of policy parameters</li>
</ul>
</li>
<li>Natural policy gradientë¥¼ ì´ìš©í•´ deterministic policiesë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<ul>
<li>$ M(\theta)^{-1}\nabla_{\theta}J(\mu_{\theta}) $ì€ any metric $ M(\theta) $ì— ëŒ€í•œ performance objective (ì‹(14))ì˜ steepest ascent direction ì…ë‹ˆë‹¤. (Toussaint, 2012)</li>
<li>Natural gradientëŠ” Fisher information metric $ M_{\pi}(\theta) $ì— ëŒ€í•œ steepest ascent direction ì…ë‹ˆë‹¤.<ul>
<li>$ M_{\pi}(\theta) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s)\nabla_{\theta}\log\pi_{\theta}(a|s)^{\top}] $</li>
<li>Fisher information metricì€ policy reparameterizationì— ëŒ€í•´ ë¶ˆë³€ì…ë‹ˆë‹¤. (Bagnell, 2003)</li>
</ul>
</li>
<li>deterministic policiesì— ëŒ€í•œ metricìœ¼ë¡œ $ M_{\mu}(\theta) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}] $ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.<ul>
<li>ì´ëŠ” varianceê°€ 0ì¸ policyì— ëŒ€í•œ Fisher information metricìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li>$ \frac{\nabla_{\theta}\pi_{\theta}(a\vert s)}{\pi_{\theta}(a\vert s)}$ì—ì„œ policy varianceê°€ 0ì´ë©´, íŠ¹ì • sì— ëŒ€í•œ $ \pi_{\theta}(a|s)$ë§Œ 1ì´ ë˜ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li>deterministic policy gradient theoremê³¼ compatible function approximationì„ ê²°í•©í•˜ë©´ $ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] $ì´ ë©ë‹ˆë‹¤.<ul>
<li>$ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $</li>
<li>$ \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)} \approx \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $</li>
</ul>
</li>
<li>ê·¸ë ‡ê¸°ì— steepest ascent directionì€ $ M_{\mu}(\theta)^{-1}\nabla_{\theta}J_{\beta}(\mu_{\theta}) = w $ì´ ë©ë‹ˆë‹¤.<ul>
<li>$ E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}]^{-1}E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] = w $</li>
</ul>
</li>
<li>ì´ ì•Œê³ ë¦¬ì¦˜ì€ COPDAC-Q í˜¹ì€ COPDAC-GQì—ì„œ $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $ ì‹ì„ $ \theta_{t+1} = \theta_{t} + \alpha_{\theta}w_{t} $ë¡œ ë°”ê¿”ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><br><br></p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h1><p><br></p>
<h2 id="5-1-Continuous-Bandit"><a href="#5-1-Continuous-Bandit" class="headerlink" title="5.1. Continuous Bandit"></a>5.1. Continuous Bandit</h2><ul>
<li>Stochastic Actor-Critic (SAC)ê³¼ COPDAC ê°„ ì„±ëŠ¥ ë¹„êµ ìˆ˜í–‰í•©ë‹ˆë‹¤.<ul>
<li>Action dimensionì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•©ë‹ˆë‹¤.</li>
<li>ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ í†µí•´ DPGì˜ data efficiencyê°€ SPGì— ë¹„í•´ ì¢‹ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, ë°˜ë©´, time-stepì´ ì¦ê°€í• ìˆ˜ë¡ SACì™€ COPDAC ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í†µí•´ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•˜ë‹¤ëŠ” ê²ƒì€ ì¼ì • time step ë‚´ì—ì„œë§Œ í•´ë‹¹í•˜ëŠ” ê²ƒì´ë¼ê³  ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><img src="https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1"></li>
</ul>
</li>
</ul>
<p><br></p>
<h2 id="5-2-Continuous-Reinforcement-Learning"><a href="#5-2-Continuous-Reinforcement-Learning" class="headerlink" title="5.2. Continuous Reinforcement Learning"></a>5.2. Continuous Reinforcement Learning</h2><ul>
<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) ê°„ ì„±ëŠ¥ ë¹„êµ ìˆ˜í–‰í•©ë‹ˆë‹¤.<ul>
<li>COPDAC-Qì˜ ì„±ëŠ¥ì´ ì•½ê°„ ë” ì¢‹ìŠµë‹ˆë‹¤.</li>
<li>COPDAC-Qì˜ í•™ìŠµì´ ë” ë¹¨ë¦¬ ì´ë¤„ì§‘ë‹ˆë‹¤.</li>
<li><img src="https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1"></li>
</ul>
</li>
</ul>
<p><br></p>
<h2 id="5-3-Octopus-Arm"><a href="#5-3-Octopus-Arm" class="headerlink" title="5.3. Octopus Arm"></a>5.3. Octopus Arm</h2><ul>
<li>ëª©í‘œ: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)ì„ controlí•˜ì—¬ targetì„ ë§ì¶”ëŠ” ê²ƒì…ë‹ˆë‹¤.<ul>
<li>COPDAC-Q ì‚¬ìš© ì‹œ, action space dimensionì´ í° octopus armì„ ì˜ controlí•˜ì—¬ targetì„ ë§ì¶¤ì…ë‹ˆë‹¤.</li>
<li><img src="https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1" width="600px"></li>
<li>ê¸°ì¡´ ê¸°ë²•ë“¤ì€ action spaces í˜¹ì€ actionê³¼ state spaces ë‘˜ ë‹¤ ì‘ì€ ê²½ìš°ë“¤ì— ëŒ€í•´ì„œë§Œ ì‹¤í—˜í–ˆë‹¤ê³  í•˜ë©°, ë¹„êµí•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.<ul>
<li>ê¸°ì¡´ ê¸°ë²•ë“¤ì´ 6 segments octopus armì—ì„œ ë™ì‘ì„ ì˜ ì•ˆ í–ˆì„ ê²ƒ ê°™ê¸´í•œë°, ê·¸ë˜ë„ ì‹¤í—˜í•´ì„œ ë³´ì—¬ì¤¬ìœ¼ë©´ í•˜ì§€ë§Œ ì‹¤í—˜ì„ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</li>
</ul>
</li>
<li>8 segment arm ë™ì˜ìƒì´ ì €ì í™ˆí˜ì´ì§€ì— ìˆë‹¤ê³  í•˜ëŠ”ë°, ì•ˆ ë³´ì…ë‹ˆë‹¤.</li>
</ul>
</li>
<li>[ì°¸ê³ ] Octopus Arm ì´ë€?<ul>
<li><a href="https://www.youtube.com/watch?v=AxeeHif0euY" target="_blank" rel="noopener">OctopusArm Youtube Link</a></li>
<li><img src="https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1"></li>
</ul>
</li>
</ul>
<p><br><br></p>
<h1 id="ì²˜ìŒìœ¼ë¡œ"><a href="#ì²˜ìŒìœ¼ë¡œ" class="headerlink" title="ì²˜ìŒìœ¼ë¡œ"></a>ì²˜ìŒìœ¼ë¡œ</h1><h2 id="PG-Travel-Guide"><a href="#PG-Travel-Guide" class="headerlink" title="PG Travel Guide"></a><a href="https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/">PG Travel Guide</a></h2><p><br></p>
<h1 id="ì´ì „ìœ¼ë¡œ"><a href="#ì´ì „ìœ¼ë¡œ" class="headerlink" title="ì´ì „ìœ¼ë¡œ"></a>ì´ì „ìœ¼ë¡œ</h1><h2 id="Sutton-PG-ì—¬í–‰í•˜ê¸°"><a href="#Sutton-PG-ì—¬í–‰í•˜ê¸°" class="headerlink" title="Sutton PG ì—¬í–‰í•˜ê¸°"></a><a href="https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/">Sutton PG ì—¬í–‰í•˜ê¸°</a></h2><p><br></p>
<h1 id="ë‹¤ìŒìœ¼ë¡œ"><a href="#ë‹¤ìŒìœ¼ë¡œ" class="headerlink" title="ë‹¤ìŒìœ¼ë¡œ"></a>ë‹¤ìŒìœ¼ë¡œ</h1><h2 id="DDPG-ì—¬í–‰í•˜ê¸°"><a href="#DDPG-ì—¬í–‰í•˜ê¸°" class="headerlink" title="DDPG ì—¬í–‰í•˜ê¸°"></a><a href="https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/">DDPG ì—¬í–‰í•˜ê¸°</a></h2>

				
			</div>

			<!-- Comments -->
			
				<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
					
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



				</div>
			
		</div>
	</div>
</article>

<!-- S ë””ìŠ¤ì¿¼ìŠ¤ ì£¼ì„ì²˜ë¦¬ ì›…ì›ì´ê°€ êº¼ë‹¬ë¼ê·¸ëŸ¼ 2018.05.10 ë³´ë¯¸ -->
<div id="disqus_thread"></div>
<script>
	

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
// (function() { // DON'T EDIT BELOW THIS LINE
// var d = document, s = d.createElement('script');
// s.src = 'https://bomee88.disqus.com/embed.js';
// s.setAttribute('data-timestamp', +new Date());
// (d.head || d.body).appendChild(s);
// })();
</script>
<!-- <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->

<!-- E ë””ìŠ¤ì¿¼ìŠ¤ ì¶”ê°€í•¨ 2018.04.09 ë³´ë¯¸-->

    <!-- Footer -->
    <!-- Footer -->
<footer>
	<div class="container">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<ul class="list-inline text-center">
					

					

					
						<li>
							<a href="https://github.com/reinforcement-learning-kr" target="_blank">
								<span class="fa-stack fa-lg">
									<i class="fa fa-circle fa-stack-2x"></i>
									<i class="fa fa-github fa-stack-1x fa-inverse"></i>
								</span>
							</a>
						</li>
					

					

					
						<li>
							<a href="mailto:rlkorea7@gmail.com" target="_blank">
								<span class="fa-stack fa-lg">
									<i class="fa fa-circle fa-stack-2x"></i>
									<i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
								</span>
							</a>
						</li>
					

					
				</ul>
				<p class="copyright text-muted">&copy; 2019 Bomi Yu</p>
			</div>
		</div>
	</div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>
<script src="/js/customizing.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Highlight -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments //ë””ìŠ¤ì¿¼ìŠ¤ ìƒˆë¡œìš´ jsì‚½ì… 2018.04.09 ë³´ë¯¸-->
<script id="dsq-count-scr" src="//bomee88.disqus.com/count.js" async></script>

<!-- // mathJax ì¶”ê°€ 2018.05.29 ë³´ë¯¸ -->
<!-- // mathJax ìˆ˜ì • 2018.06.15 ì›…ì› -->
<!--
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$']]
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML">
</script>
                          

<script type="text/javascript">
    var disqus_shortname = 'rlkorea';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</body>

</html>